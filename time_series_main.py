# -*- coding: utf-8 -*-
"""time_series_main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TDoXdmtrqehLj0s7ASf-IJZlH99pNpQe
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
import warnings

warnings.filterwarnings('ignore')
np.random.seed(42)

# Generate synthetic stock-like data
def generate_data(n=10000):
    t = np.arange(n)
    trend = 0.001 * t
    seasonality = 10 * np.sin(2 * np.pi * t / 20)
    noise = 0.02 * np.random.randn(n)
    return 100 + trend + seasonality + noise

# Create training sequences
def make_sequences(data, lookback=30):
    X, y = [], []
    for i in range(len(data) - lookback):
        X.append(data[i:i+lookback])
        y.append(data[i+lookback])
    return np.array(X), np.array(y)

# Load and prep data
raw_data = generate_data(n=1000)
scaler = MinMaxScaler()
data = scaler.fit_transform(raw_data.reshape(-1, 1)).flatten()

lookback = 30
X, y = make_sequences(data, lookback=lookback)
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

print(f"Training set: {X_train.shape}, Test set: {X_test.shape}")

# Naive baseline - just use last value
class LastValueModel:
    def predict(self, X):
        return X[:, -1]

# Simple AR model
class SimpleAR:
    def __init__(self, p=5):
        self.p = p
        self.lr = LinearRegression()

    def fit(self, y):
        X_train = np.array([y[i-self.p:i] for i in range(self.p, len(y))])
        y_train = y[self.p:]
        self.lr.fit(X_train, y_train)
        return self

    def predict(self, X):
        return self.lr.predict(X)

# Train baselines
last_val = LastValueModel()
ar = SimpleAR(p=lookback).fit(y_train)

y_pred_naive = last_val.predict(X_test)
y_pred_ar = ar.predict(X_test)

# Try LSTM if available
try:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping

    X_train_3d = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test_3d = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

    model = Sequential([
        LSTM(64, activation='relu', input_shape=(lookback, 1), return_sequences=True),
        Dropout(0.2),
        LSTM(32, activation='relu'),
        Dropout(0.2),
        Dense(16, activation='relu'),
        Dense(1)
    ])

    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    model.fit(X_train_3d, y_train, epochs=25, batch_size=32,
              validation_split=0.15, callbacks=[EarlyStopping(monitor='val_loss', patience=5)],
              verbose=0)

    y_pred_lstm = model.predict(X_test_3d, verbose=0).flatten()
    lstm_available = True

except ImportError:
    print("TensorFlow not installed - skipping LSTM")
    lstm_available = False

# Evaluate
def get_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return {'rmse': rmse, 'mae': mae, 'r2': r2}

results = {}
results['Naive'] = get_metrics(y_test, y_pred_naive)
results['AR(5)'] = get_metrics(y_test, y_pred_ar)

if lstm_available:
    results['LSTM'] = get_metrics(y_test, y_pred_lstm)

print("\n--- Results ---")
for model_name, metrics in results.items():
    print(f"{model_name}: RMSE={metrics['rmse']:.4f}, MAE={metrics['mae']:.4f}, R2={metrics['r2']:.4f}")

# Plotting
fig, axes = plt.subplots(2, 2, figsize=(13, 9))

# Full time series
axes[0, 0].plot(data, linewidth=1, alpha=0.8)
axes[0, 0].axvline(split + lookback, color='red', linestyle='--', alpha=0.7)
axes[0, 0].set_title('Time Series Data')
axes[0, 0].set_ylabel('Normalized Value')
axes[0, 0].grid(alpha=0.3)

# Test predictions
t = np.arange(len(y_test))
axes[0, 1].plot(t, y_test, 'o-', label='Actual', linewidth=2, markersize=4)
axes[0, 1].plot(t, y_pred_naive, '--', label='Naive', alpha=0.7)
axes[0, 1].plot(t, y_pred_ar, '--', label='AR(5)', alpha=0.7)
if lstm_available:
    axes[0, 1].plot(t, y_pred_lstm, '--', label='LSTM', alpha=0.7)
axes[0, 1].set_title('Test Predictions')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# Residuals
pred = y_pred_lstm if lstm_available else y_pred_ar
residuals = y_test - pred
axes[1, 0].hist(residuals, bins=25, edgecolor='black', alpha=0.7)
axes[1, 0].axvline(0, color='red', linestyle='--')
axes[1, 0].set_title('Residuals Distribution')
axes[1, 0].set_xlabel('Error')
axes[1, 0].grid(alpha=0.3, axis='y')

# Model comparison
models = list(results.keys())
rmses = [results[m]['rmse'] for m in models]
axes[1, 1].barh(models, rmses, color='steelblue', alpha=0.8)
axes[1, 1].set_xlabel('RMSE')
axes[1, 1].set_title('Model Comparison')
axes[1, 1].invert_yaxis()

plt.tight_layout()
plt.savefig('results.png', dpi=300, bbox_inches='tight')
print("\nPlot saved as results.png")

# Future prediction function
def forecast(last_seq, steps=10):
    preds = []
    seq = last_seq.copy()
    for _ in range(steps):
        if lstm_available:
            next_val = model.predict(seq.reshape(1, lookback, 1), verbose=0)[0, 0]
        else:
            next_val = ar.predict(seq.reshape(1, -1))[0]
        preds.append(next_val)
        seq = np.append(seq[1:], next_val)
    return scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()

future = forecast(X_test[-1], steps=10)
print("\nNext 10 steps forecast:")
for i, val in enumerate(future, 1):
    print(f"  Step {i}: {val:.2f}")